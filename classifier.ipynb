{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9H4gMCBSFiQ",
        "outputId": "131dce7c-721f-4ae7-f2ac-809feb1972a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Neural Networks libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, LSTM, Dropout, Bidirectional,Flatten\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatizer\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "# Checks if given word contains a special character\n",
        "def contains_special(word):\n",
        "    for char in word:\n",
        "        if char.isnumeric() or (not char.isalnum()):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# process sentences\n",
        "def process_sentences(sentence):\n",
        "    # tokenize, lemmatize, and remove special characters\n",
        "    processed = [lm.lemmatize(word.lower()) for word in nltk.word_tokenize(sentence)\n",
        "          # make sure no strings that contain only numeric characters\n",
        "          if not contains_special(word)]\n",
        "    return ' '.join(processed)"
      ],
      "metadata": {
        "id": "DOkUpI-WSH8n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "df = pd.read_csv('tweets.csv')\n",
        "# drop duplicates\n",
        "df.drop_duplicates(inplace =True)\n",
        "X = df['text']\n",
        "y = df['target']"
      ],
      "metadata": {
        "id": "52uaa-DwYYTR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbN4bruno61m",
        "outputId": "340a4e47-ba3e-459a-e77a-ce113b8c9db3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    9256\n",
              "1    2114\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text pre-processing\n",
        "X = X.apply(process_sentences)"
      ],
      "metadata": {
        "id": "qZLW4iIRYHVX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "g10dbPQH_hL3",
        "outputId": "1812e5ba-a065-4698-a4b3-064a85f385e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'communal violence in bhainsa telangana stone were pelted on muslim house and some house and vehicle were set'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text data into BoW features\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X)\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "print('Vocab size: ' + str(len(vocabulary)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIA1b_qm-HJ9",
        "outputId": "33e6f607-2d45-4ac7-c01f-eceaeb452ced"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 16985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary['house']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq0a9ylnZJ53",
        "outputId": "0498f5a1-1da4-4615-a682-52ac37fc7980"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6955"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "6BKU8gR-eghi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "def naive_bayes(X_train, X_test, y_train, y_test):\n",
        "    # increased max iter to prevent early time out\n",
        "    clf = MultinomialNB().fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print('accuracy:', metrics.accuracy_score(y_test, y_pred))\n",
        "    print(metrics.classification_report(y_test,\n",
        "                                    y_pred))\n",
        "    return y_pred\n",
        "\n",
        "print('Naive Bayes\\n')\n",
        "ypred_lr = naive_bayes(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "Cq-bGF8QoqZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "def logistic_regression(X_train, X_test, y_train, y_test):\n",
        "    scaler = StandardScaler()\n",
        "    # scale data to reduce ranges\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    # increased max iter to prevent early time out\n",
        "    clf = LogisticRegression(max_iter = 500).fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print('accuracy:', metrics.accuracy_score(y_test, y_pred))\n",
        "    print(metrics.classification_report(y_test,\n",
        "                                    y_pred))\n",
        "    return y_pred\n",
        "\n",
        "print('Logistic Regression\\n')\n",
        "ypred_lr = logistic_regression(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "yT6YA0Qjfl3a",
        "outputId": "5aa4ab9b-264d-4e20-db91-5c4ed6d0c502"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b86f4c9d1487>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logistic Regression\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mypred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "df = pd.read_csv('tweets.csv')\n",
        "# drop duplicates\n",
        "df.drop_duplicates(inplace =True)\n",
        "\n",
        "X = df['text']\n",
        "y = df['target']\n",
        "\n",
        "# text pre-pro|cessing\n",
        "X = X.apply(process_sentences)"
      ],
      "metadata": {
        "id": "zp81tAftC670"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "def get_embeddings(data, size):\n",
        "  return Word2Vec(sentences=data, vector_size=size, window=5, min_count=1,\n",
        "                 sg=1)\n",
        "\n",
        "# get a mapping of word -> embedding, word_index -> embedding\n",
        "def map_embeddings(model, tokenizer):\n",
        "    # all vocab\n",
        "    vocab = list(model.wv.index_to_key)\n",
        "    # word to word index map\n",
        "    word_index = tokenizer.word_index\n",
        "    # word to embedding map\n",
        "    word_embed = {}\n",
        "    # word index to embedding map\n",
        "    index_embed = {}\n",
        "\n",
        "    for w in vocab:\n",
        "        # get current word's embedding\n",
        "        e = model.wv[w]\n",
        "        # set embedding for current word\n",
        "        word_embed[w] = e\n",
        "        # set embedding for current word index\n",
        "        curr_index = word_index[w]\n",
        "        index_embed[curr_index] = e\n",
        "    return word_embed, index_embed\n",
        "\n",
        "# pre-padding data to fixed length token for NN input\n",
        "def pre_padding(encoded, seq_length, model):\n",
        "    if model == \"rnn\":\n",
        "      X = []\n",
        "      for row in encoded:\n",
        "          for i in range(1, len(row) - 1):\n",
        "              X.append(row[:i])\n",
        "    X = pad_sequences(encoded, maxlen = seq_length)\n",
        "    return X\n",
        "\n",
        "# convert data into 3D matrix (total rows, max sequence length, embedding size) for LSTM\n",
        "def reshape_data(X, index_embed):\n",
        "  reshaped = []\n",
        "  for seq in X:\n",
        "    # get embeddings for each word index in sequence\n",
        "    seq_embed = [index_embed[index] for index in seq]\n",
        "    reshaped.append(seq_embed)\n",
        "  # convert to numpy array\n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "def data_generator(X, y, num_sequences, index_embed, model) -> (list,list):\n",
        "    i = 0\n",
        "    while i < len(X):\n",
        "        # end range of data for current batch\n",
        "        end_index = i + num_sequences\n",
        "        # reached end of our dataset\n",
        "        if end_index >= len(X) - 1:\n",
        "            i = 0\n",
        "            end_index = i + num_sequences\n",
        "\n",
        "        if model == \"ffnn\":\n",
        "          # we need to flatten our inputs for a feedforward network\n",
        "          inputs = [input.flatten() for input in X[i:end_index]]\n",
        "        else:\n",
        "          inputs = [val for val in X[i:end_index]]\n",
        "\n",
        "        outputs = [val for val in y[i:end_index]]\n",
        "        yield np.array(inputs), np.array(outputs)\n",
        "        i += num_sequences\n"
      ],
      "metadata": {
        "id": "tb6nyfLbjoJI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process data for Gensim's Word2Vec\n",
        "X = X.apply(nltk.word_tokenize)\n",
        "word2vec = get_embeddings(X,200)\n",
        "vocab_size = len(word2vec.wv.index_to_key)\n",
        "print('Vocab size {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xwix9aM_lVO",
        "outputId": "493e004e-cc2f-4f08-8082-a3359b967381"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size 17020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.wv.most_similar('people', topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAqRuWvP_o2m",
        "outputId": "0e719641-0081-460f-9bf9-8d4ebc862cb5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('those', 0.9485560059547424),\n",
              " ('these', 0.9232138395309448),\n",
              " ('their', 0.905163586139679),\n",
              " ('many', 0.8945909738540649),\n",
              " ('american', 0.8919721841812134),\n",
              " ('innocent', 0.886820375919342),\n",
              " ('who', 0.8828123211860657),\n",
              " ('asking', 0.8800298571586609),\n",
              " ('other', 0.8794156312942505),\n",
              " ('animal', 0.8793823719024658)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "embedding_size = 200\n",
        "# embeddings\n",
        "word_embed, index_embed = map_embeddings(word2vec, tokenizer)\n",
        "# set embeddings for zero index\n",
        "index_embed[0] = np.zeros((embedding_size,))\n",
        "word_embed[''] = np.zeros((embedding_size,))\n",
        "vocab_size = len(word_embed.keys())"
      ],
      "metadata": {
        "id": "pSHlW9DS41pA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode data into word indexes\n",
        "encoded_data = tokenizer.texts_to_sequences(X)\n",
        "# max sequence length\n",
        "max_sequence_length = np.max([len(seq) for seq in encoded_data])\n",
        "# prepadding on data\n",
        "ffnn_pre_padded_X = pre_padding(encoded_data, max_sequence_length, \"ffnn\")\n",
        "rnn_pre_padded_X = pre_padding(encoded_data, max_sequence_length, \"rnn\")"
      ],
      "metadata": {
        "id": "xziXPGKh_oeM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert pre-padded and encoded data into 3D shape for FFNN input\n",
        "ffnn_X = reshape_data(ffnn_pre_padded_X, index_embed)\n",
        "print(ffnn_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGCSVHXWHM9a",
        "outputId": "189ffb6f-86dd-44ad-f602-6bfd47113e96"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11370, 30, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert pre-padded and encoded data into 3D shape for FFNN input\n",
        "rnn_X = reshape_data(rnn_pre_padded_X, index_embed)\n",
        "print(rnn_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBVKNs3HZODW",
        "outputId": "58507f99-20f6-4c66-a167-1ed8f8d25652"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11370, 30, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(ffnn_X, y, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "9C-f-may_evj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward Neural Network\n",
        "# hyperparameters\n",
        "epochs = 10\n",
        "batches = 256\n",
        "steps = len(X_train)//batches\n",
        "\n",
        "# data generator\n",
        "train_generator = data_generator(X_train, y_train, batches, index_embed, \"ffnn\")\n",
        "\n",
        "# architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim = max_sequence_length * embedding_size))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "# sigmoid for binary classification\n",
        "model.add(Dense(units= 1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x= train_generator,\n",
        "          steps_per_epoch= steps,\n",
        "          epochs= epochs, verbose = 1)\n",
        "\n",
        "print(model.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyQgSocPNyQn",
        "outputId": "53c1f7dd-9c6b-44fb-9d1e-50721494f9d5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "33/33 [==============================] - 3s 47ms/step - loss: 0.4298 - accuracy: 0.8162\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 2s 47ms/step - loss: 0.3635 - accuracy: 0.8499\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 2s 46ms/step - loss: 0.3460 - accuracy: 0.8581\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 2s 47ms/step - loss: 0.3354 - accuracy: 0.8614\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 3s 83ms/step - loss: 0.3249 - accuracy: 0.8691\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 3s 76ms/step - loss: 0.3171 - accuracy: 0.8714\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 2s 71ms/step - loss: 0.3128 - accuracy: 0.8717\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.3091 - accuracy: 0.8751\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 2s 47ms/step - loss: 0.2997 - accuracy: 0.8808\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 2s 46ms/step - loss: 0.2906 - accuracy: 0.8833\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_39/Sigmoid:0', description=\"created by layer 'dense_39'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(rnn_X, y, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "SuedOlc-Zs9l"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "# hyperparameters\n",
        "epochs = 5\n",
        "batches = 256\n",
        "steps = len(X_train)//batches\n",
        "\n",
        "# Data generator\n",
        "train_generator = data_generator(X_train, y_train, batches, index_embed, \"rnn\")\n",
        "\n",
        "# Architecture\n",
        "model = Sequential()\n",
        "# LSTM input layer\n",
        "model.add(LSTM(200, input_shape=(max_sequence_length, embedding_size),return_sequences=True))\n",
        "# Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "# LSTM layer\n",
        "model.add(LSTM(128, input_shape=(max_sequence_length, embedding_size),return_sequences=True))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.2))\n",
        "# sigmoid for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x= train_generator,\n",
        "          steps_per_epoch= steps,\n",
        "          epochs= epochs, verbose = 1)\n",
        "\n",
        "print(model.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJY0rrrWM43L",
        "outputId": "91872339-938b-4e93-8a21-56822e2a7348"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "33/33 [==============================] - 29s 691ms/step - loss: 0.4355 - accuracy: 0.8252\n",
            "Epoch 2/5\n",
            "33/33 [==============================] - 20s 623ms/step - loss: 0.3662 - accuracy: 0.8475\n",
            "Epoch 3/5\n",
            "33/33 [==============================] - 21s 648ms/step - loss: 0.3566 - accuracy: 0.8513\n",
            "Epoch 4/5\n",
            "33/33 [==============================] - 22s 642ms/step - loss: 0.3496 - accuracy: 0.8546\n",
            "Epoch 5/5\n",
            "33/33 [==============================] - 21s 624ms/step - loss: 0.3458 - accuracy: 0.8544\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_42/Sigmoid:0', description=\"created by layer 'dense_42'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrent Neural Networks : Bidirectional LSTM\n",
        "# hyperparameters\n",
        "epochs = 5\n",
        "batches = 256\n",
        "steps = len(X_train)//batches\n",
        "\n",
        "# Data generator\n",
        "train_generator = data_generator(X_train, y_train, batches, index_embed, \"rnn\")\n",
        "\n",
        "# Model architecture\n",
        "model = Sequential()\n",
        "# LSTM input layer\n",
        "model.add(LSTM(128, input_shape=(max_sequence_length, embedding_size),return_sequences=True))\n",
        "model.add(LSTM(64, input_shape=(max_sequence_length, embedding_size),return_sequences=True))\n",
        "# Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "# Bidirectional LSTM layer for extra context\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.2))\n",
        "# sigmoid for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x= train_generator,\n",
        "          steps_per_epoch= steps,\n",
        "          epochs= epochs, verbose = 1)\n",
        "\n",
        "print(model.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ5vAYa3JtIx",
        "outputId": "36b7c9c5-d1a3-493f-b880-11b752d9027f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "33/33 [==============================] - 21s 362ms/step - loss: 0.4967 - accuracy: 0.8037\n",
            "Epoch 2/5\n",
            "33/33 [==============================] - 13s 395ms/step - loss: 0.3864 - accuracy: 0.8365\n",
            "Epoch 3/5\n",
            "33/33 [==============================] - 13s 404ms/step - loss: 0.3629 - accuracy: 0.8519\n",
            "Epoch 4/5\n",
            "33/33 [==============================] - 14s 415ms/step - loss: 0.3528 - accuracy: 0.8536\n",
            "Epoch 5/5\n",
            "33/33 [==============================] - 21s 636ms/step - loss: 0.3461 - accuracy: 0.8570\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_43/Sigmoid:0', description=\"created by layer 'dense_43'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UlboNNO0KvcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}